"""
æµ‹è¯• Content Modality Analyzer

æµ‹è¯•å†…å®¹ï¼š
1. æ¨¡æ€åˆ†æ
2. éŸ³é¢‘åŒ¹é…
3. å®Œæ•´æµæ°´çº¿
"""
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from app.tools.modality_analyzer import (
    ModalityAnalyzer,
    analyze_modality,
    should_run_vision,
    ModalityAnalysis
)
from app.tools.audio_matcher import AudioMatcher, match_audio_to_videos
from app.tools.smart_pipeline import SmartPipeline


def test_modality_analyzer():
    """æµ‹è¯• 1: æ¨¡æ€åˆ†æå™¨"""
    print("\n" + "="*60)
    print("æµ‹è¯• 1: æ¨¡æ€åˆ†æå™¨")
    print("="*60)
    
    analyzer = ModalityAnalyzer()
    
    # æµ‹è¯•ç”¨ä¾‹ 1: å£æ’­è§†é¢‘ï¼ˆæ¨¡æ‹Ÿï¼‰
    print("\nğŸ“¹ æµ‹è¯•ç”¨ä¾‹ 1: å£æ’­è§†é¢‘")
    analysis = ModalityAnalysis(
        has_voice=True,
        speech_ratio=0.78,
        music_ratio=0.12,
        silence_ratio=0.10,
        likely_talking_head=True,
        recommended_mode="ASR_PRIMARY",
        confidence=0.9,
        audio_present=True,
        avg_volume_db=-20,
        volume_variance=12,
        speech_segments=45
    )
    
    print(f"âœ“ æœ‰è¯­éŸ³: {analysis.has_voice}")
    print(f"âœ“ è¯­éŸ³å æ¯”: {analysis.speech_ratio*100:.1f}%")
    print(f"âœ“ éŸ³ä¹å æ¯”: {analysis.music_ratio*100:.1f}%")
    print(f"âœ“ é™éŸ³å æ¯”: {analysis.silence_ratio*100:.1f}%")
    print(f"âœ“ å¯èƒ½æ˜¯å£æ’­: {analysis.likely_talking_head}")
    print(f"âœ“ æ¨èæ¨¡å¼: {analysis.recommended_mode}")
    print(f"âœ“ ç½®ä¿¡åº¦: {analysis.confidence*100:.1f}%")
    
    # æµ‹è¯•ç”¨ä¾‹ 2: B-rollï¼ˆæ— å£°ï¼‰
    print("\nğŸ“¹ æµ‹è¯•ç”¨ä¾‹ 2: B-rollï¼ˆæ— å£°ï¼‰")
    analysis2 = ModalityAnalysis(
        has_voice=False,
        speech_ratio=0.0,
        music_ratio=0.0,
        silence_ratio=1.0,
        likely_talking_head=False,
        recommended_mode="VISION_PRIMARY",
        confidence=0.95,
        audio_present=False,
        avg_volume_db=-100,
        volume_variance=0,
        speech_segments=0
    )
    
    print(f"âœ“ æœ‰è¯­éŸ³: {analysis2.has_voice}")
    print(f"âœ“ æ¨èæ¨¡å¼: {analysis2.recommended_mode}")
    print(f"âœ“ ç½®ä¿¡åº¦: {analysis2.confidence*100:.1f}%")
    
    # æµ‹è¯•ç”¨ä¾‹ 3: Vlogï¼ˆæ··åˆï¼‰
    print("\nğŸ“¹ æµ‹è¯•ç”¨ä¾‹ 3: Vlogï¼ˆæ··åˆï¼‰")
    analysis3 = ModalityAnalysis(
        has_voice=True,
        speech_ratio=0.45,
        music_ratio=0.25,
        silence_ratio=0.30,
        likely_talking_head=False,
        recommended_mode="HYBRID",
        confidence=0.7,
        audio_present=True,
        avg_volume_db=-18,
        volume_variance=8,
        speech_segments=20
    )
    
    print(f"âœ“ æœ‰è¯­éŸ³: {analysis3.has_voice}")
    print(f"âœ“ è¯­éŸ³å æ¯”: {analysis3.speech_ratio*100:.1f}%")
    print(f"âœ“ æ¨èæ¨¡å¼: {analysis3.recommended_mode}")
    print(f"âœ“ ç½®ä¿¡åº¦: {analysis3.confidence*100:.1f}%")


def test_should_run_vision():
    """æµ‹è¯• 2: Vision è¿è¡Œåˆ¤æ–­"""
    print("\n" + "="*60)
    print("æµ‹è¯• 2: Vision è¿è¡Œåˆ¤æ–­")
    print("="*60)
    
    # åœºæ™¯ 1: ASR_PRIMARY + æœ‰è½¬å½• â†’ ä¸è·‘ Vision
    print("\nğŸ“‹ åœºæ™¯ 1: ASR_PRIMARY + æœ‰è½¬å½•")
    modality = ModalityAnalysis(
        has_voice=True,
        speech_ratio=0.78,
        music_ratio=0.12,
        silence_ratio=0.10,
        likely_talking_head=True,
        recommended_mode="ASR_PRIMARY",
        confidence=0.9,
        audio_present=True,
        avg_volume_db=-20,
        volume_variance=12,
        speech_segments=45
    )
    
    should_run = should_run_vision(modality, segment_has_transcript=True, transcript_confidence=0.9)
    print(f"âœ“ æ˜¯å¦è¿è¡Œ Vision: {should_run} (é¢„æœŸ: False)")
    
    # åœºæ™¯ 2: ASR_PRIMARY + æ— è½¬å½• â†’ è·‘ Vision
    print("\nğŸ“‹ åœºæ™¯ 2: ASR_PRIMARY + æ— è½¬å½•")
    should_run = should_run_vision(modality, segment_has_transcript=False)
    print(f"âœ“ æ˜¯å¦è¿è¡Œ Vision: {should_run} (é¢„æœŸ: True)")
    
    # åœºæ™¯ 3: VISION_PRIMARY â†’ å¿…é¡»è·‘ Vision
    print("\nğŸ“‹ åœºæ™¯ 3: VISION_PRIMARY")
    modality2 = ModalityAnalysis(
        has_voice=False,
        speech_ratio=0.0,
        music_ratio=0.0,
        silence_ratio=1.0,
        likely_talking_head=False,
        recommended_mode="VISION_PRIMARY",
        confidence=0.95,
        audio_present=False,
        avg_volume_db=-100,
        volume_variance=0,
        speech_segments=0
    )
    
    should_run = should_run_vision(modality2, segment_has_transcript=True)
    print(f"âœ“ æ˜¯å¦è¿è¡Œ Vision: {should_run} (é¢„æœŸ: True)")
    
    # åœºæ™¯ 4: HYBRID + ä½ç½®ä¿¡åº¦è½¬å½• â†’ è·‘ Vision
    print("\nğŸ“‹ åœºæ™¯ 4: HYBRID + ä½ç½®ä¿¡åº¦è½¬å½•")
    modality3 = ModalityAnalysis(
        has_voice=True,
        speech_ratio=0.45,
        music_ratio=0.25,
        silence_ratio=0.30,
        likely_talking_head=False,
        recommended_mode="HYBRID",
        confidence=0.7,
        audio_present=True,
        avg_volume_db=-18,
        volume_variance=8,
        speech_segments=20
    )
    
    should_run = should_run_vision(modality3, segment_has_transcript=True, transcript_confidence=0.5)
    print(f"âœ“ æ˜¯å¦è¿è¡Œ Vision: {should_run} (é¢„æœŸ: True)")


def test_audio_matcher():
    """æµ‹è¯• 3: éŸ³é¢‘åŒ¹é…å™¨"""
    print("\n" + "="*60)
    print("æµ‹è¯• 3: éŸ³é¢‘åŒ¹é…å™¨")
    print("="*60)
    
    matcher = AudioMatcher()
    
    # æ¨¡æ‹Ÿè§†é¢‘å’ŒéŸ³é¢‘èµ„æº
    videos = [
        {
            "asset_id": "V001",
            "path": "D:/footage/A001.mp4",
            "filename": "A001.mp4"
        },
        {
            "asset_id": "V002",
            "path": "D:/footage/B002.mp4",
            "filename": "B002.mp4"
        }
    ]
    
    audios = [
        {
            "asset_id": "A001",
            "path": "D:/footage/A001.wav",
            "filename": "A001.wav"
        },
        {
            "asset_id": "A002",
            "path": "D:/footage/C003.wav",
            "filename": "C003.wav"
        }
    ]
    
    # æµ‹è¯•æ˜¾å¼åŒ¹é…
    print("\nğŸµ æµ‹è¯•æ˜¾å¼åŒ¹é…")
    match = matcher._explicit_match(videos[0], audios)
    if match:
        print(f"âœ“ V001 åŒ¹é…åˆ°: {match['asset_id']} (æ–¹æ³•: æ–‡ä»¶ååŒ¹é…)")
    else:
        print(f"âœ“ V001 æ— åŒ¹é…")
    
    match = matcher._explicit_match(videos[1], audios)
    if match:
        print(f"âœ“ V002 åŒ¹é…åˆ°: {match['asset_id']}")
    else:
        print(f"âœ“ V002 æ— åŒ¹é… (é¢„æœŸ)")


def test_decision_matrix():
    """æµ‹è¯• 4: å†³ç­–çŸ©é˜µ"""
    print("\n" + "="*60)
    print("æµ‹è¯• 4: å†³ç­–çŸ©é˜µ")
    print("="*60)
    
    analyzer = ModalityAnalyzer()
    
    # æµ‹è¯•å„ç§åœºæ™¯
    test_cases = [
        {
            "name": "å‡ºé•œå£æ’­",
            "has_voice": True,
            "speech_ratio": 0.85,
            "music_ratio": 0.05,
            "silence_ratio": 0.10,
            "likely_talking_head": True,
            "expected": "ASR_PRIMARY"
        },
        {
            "name": "æ•™ç¨‹è§£è¯´",
            "has_voice": True,
            "speech_ratio": 0.75,
            "music_ratio": 0.10,
            "silence_ratio": 0.15,
            "likely_talking_head": True,
            "expected": "ASR_PRIMARY"
        },
        {
            "name": "Vlog",
            "has_voice": True,
            "speech_ratio": 0.50,
            "music_ratio": 0.20,
            "silence_ratio": 0.30,
            "likely_talking_head": False,
            "expected": "HYBRID"
        },
        {
            "name": "äº§å“å±•ç¤º",
            "has_voice": True,
            "speech_ratio": 0.20,
            "music_ratio": 0.60,
            "silence_ratio": 0.20,
            "likely_talking_head": False,
            "expected": "VISION_PRIMARY"
        },
        {
            "name": "B-roll",
            "has_voice": False,
            "speech_ratio": 0.0,
            "music_ratio": 0.0,
            "silence_ratio": 1.0,
            "likely_talking_head": False,
            "expected": "VISION_PRIMARY"
        }
    ]
    
    print("\nå†³ç­–çŸ©é˜µæµ‹è¯•:")
    print("-" * 60)
    
    for case in test_cases:
        mode, confidence = analyzer._decide_mode(
            case["has_voice"],
            case["speech_ratio"],
            case["music_ratio"],
            case["silence_ratio"],
            case["likely_talking_head"]
        )
        
        match = "âœ“" if mode == case["expected"] else "âœ—"
        print(f"{match} {case['name']:12s} â†’ {mode:16s} (ç½®ä¿¡åº¦: {confidence*100:.0f}%)")


def test_smart_pipeline():
    """æµ‹è¯• 5: å®Œæ•´æµæ°´çº¿ï¼ˆæ¨¡æ‹Ÿï¼‰"""
    print("\n" + "="*60)
    print("æµ‹è¯• 5: å®Œæ•´æµæ°´çº¿ï¼ˆæ¨¡æ‹Ÿï¼‰")
    print("="*60)
    
    # åˆ›å»ºä¸´æ—¶ job ç›®å½•
    job_dir = Path(__file__).parent / "test_output" / "modality_test"
    job_dir.mkdir(parents=True, exist_ok=True)
    
    print(f"\nğŸ“ Job ç›®å½•: {job_dir}")
    
    # æ¨¡æ‹Ÿè¾“å…¥æ–‡ä»¶ï¼ˆä¸å®é™…è¿è¡Œï¼Œåªæµ‹è¯•æµç¨‹ï¼‰
    print("\nâœ“ æµæ°´çº¿æ­¥éª¤:")
    print("  1. Ingest & Index")
    print("  2. Quick Quality Triage")
    print("  3. Match Audio to Video")
    print("  4. Modality Analysis")
    print("  5. Segment Assets")
    print("  6A. ASR Recognition")
    print("  6B. Vision Analysis (selective)")
    print("  6C. Structure Vision Data")
    print("  7. Generate ShotCards")
    
    print("\nâœ“ æµæ°´çº¿è®¾è®¡å®Œæˆ")


def main():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("\n" + "="*60)
    print("Content Modality Analyzer æµ‹è¯•")
    print("="*60)
    
    try:
        # æµ‹è¯• 1: æ¨¡æ€åˆ†æå™¨
        test_modality_analyzer()
        
        # æµ‹è¯• 2: Vision è¿è¡Œåˆ¤æ–­
        test_should_run_vision()
        
        # æµ‹è¯• 3: éŸ³é¢‘åŒ¹é…å™¨
        test_audio_matcher()
        
        # æµ‹è¯• 4: å†³ç­–çŸ©é˜µ
        test_decision_matrix()
        
        # æµ‹è¯• 5: å®Œæ•´æµæ°´çº¿
        test_smart_pipeline()
        
        print("\n" + "="*60)
        print("âœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡")
        print("="*60 + "\n")
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0


if __name__ == "__main__":
    exit(main())
