"""
Smart Pipeline - æ™ºèƒ½å¤„ç†æµæ°´çº¿

å®Œæ•´æµç¨‹ï¼š
Step 0: æ¨¡æ€åˆ†æï¼ˆè¶…å¿«ï¼‰
Step 1: ç²—åˆ‡é•œå¤´ï¼ˆè½»é‡ï¼‰
Step 2A: ASR ä¸»è·¯å¾„ï¼ˆå¤§å¤šæ•°æƒ…å†µï¼‰
Step 2B: Vision è¡¥å……è·¯å¾„ï¼ˆåªåœ¨å¿…è¦æ—¶ï¼‰
Step 3: èåˆç”Ÿæˆ ShotCards

ä¸»æµç¨‹ï¼šIngest â†’ Triage â†’ Segment â†’ ASR/Vision â†’ Fuse
"""
from pathlib import Path
from typing import Dict, Any, List, Optional
import json

from .modality_analyzer import ModalityAnalyzer, should_run_vision
from .audio_matcher import AudioMatcher


class SmartPipeline:
    """æ™ºèƒ½å¤„ç†æµæ°´çº¿"""
    
    def __init__(self, job_dir: Path):
        self.job_dir = job_dir
        self.input_dir = job_dir / "input"
        self.output_dir = job_dir / "output"
        self.temp_dir = job_dir / "temp"
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        self.input_dir.mkdir(exist_ok=True)
        self.output_dir.mkdir(exist_ok=True)
        self.temp_dir.mkdir(exist_ok=True)
        
        # åˆå§‹åŒ–åˆ†æå™¨
        self.modality_analyzer = ModalityAnalyzer()
        self.audio_matcher = AudioMatcher()
    
    def run(self, input_paths: List[str]) -> Dict[str, Any]:
        """
        è¿è¡Œå®Œæ•´æµæ°´çº¿
        
        Args:
            input_paths: è¾“å…¥æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        
        Returns:
            å¤„ç†ç»“æœ
        """
        print("\n" + "="*60)
        print("ğŸš€ Smart Pipeline å¯åŠ¨")
        print("="*60)
        
        # Step 1: Ingest & Index
        print("\nğŸ“¦ Step 1: Ingest & Index")
        assets = self._build_assets_manifest(input_paths)
        self._save_json("assets_manifest.json", assets)
        print(f"âœ“ å‘ç° {len(assets['videos'])} ä¸ªè§†é¢‘, {len(assets['audios'])} ä¸ªéŸ³é¢‘")
        
        # Step 2: Triage (cheap quality check)
        print("\nğŸ” Step 2: Quick Quality Triage")
        assets = self._quick_quality_triage(assets)
        self._save_json("assets_manifest_with_triage.json", assets)
        usable_count = sum(1 for v in assets['videos'] if v.get('quality', {}).get('usable', True))
        print(f"âœ“ {usable_count}/{len(assets['videos'])} ä¸ªè§†é¢‘å¯ç”¨")
        
        # Step 3: Match external audio to video
        print("\nğŸµ Step 3: Match Audio to Video")
        assets = self._match_audio_to_video(assets)
        self._save_json("assets_manifest_with_matching.json", assets)
        matched_count = sum(1 for v in assets['videos'] if v.get('matched_audio_asset_id'))
        print(f"âœ“ {matched_count} ä¸ªè§†é¢‘åŒ¹é…åˆ°å¤–éƒ¨éŸ³é¢‘")
        
        # Step 4: Decide modality per asset
        print("\nğŸ§  Step 4: Modality Analysis")
        policies = self._decide_modality_policies(assets)
        self._save_json("modality_policy.json", policies)
        self._print_modality_summary(policies)
        
        # Step 5: Segment assets
        print("\nâœ‚ï¸  Step 5: Segment Assets")
        segments = self._segment_assets(assets, policies)
        self._save_json("segments.json", segments)
        print(f"âœ“ ç”Ÿæˆ {len(segments)} ä¸ªå¯å‰ªè¾‘æ®µ")
        
        # Step 6A: ASR pass
        print("\nğŸ¤ Step 6A: ASR Recognition")
        transcripts = self._run_asr_pass(segments, policies)
        self._save_json("transcripts.json", transcripts)
        print(f"âœ“ è½¬å½• {len(transcripts)} ä¸ªè¯­éŸ³æ®µ")
        
        # Step 6B: Vision pass (only when needed)
        print("\nğŸ‘ï¸  Step 6B: Vision Analysis (selective)")
        vision_caps = self._run_vision_pass(segments, policies, transcripts)
        self._save_json("vision_captions.json", vision_caps)
        print(f"âœ“ åˆ†æ {len(vision_caps)} ä¸ªè§†è§‰æ®µ")
        
        # Step 6C: Cloud structuring
        print("\nğŸ§  Step 6C: Structure Vision Data")
        vision_meta = self._structure_vision_data(vision_caps)
        self._save_json("vision_meta.json", vision_meta)
        print(f"âœ“ ç»“æ„åŒ– {len(vision_meta)} ä¸ªè§†è§‰å…ƒæ•°æ®")
        
        # Step 7: Fuse into ShotCards
        print("\nğŸ¬ Step 7: Generate ShotCards")
        shotcards = self._generate_shotcards(segments, transcripts, vision_meta, assets)
        self._save_json("shotcards.json", shotcards)
        print(f"âœ“ ç”Ÿæˆ {len(shotcards)} ä¸ª ShotCard")
        
        print("\n" + "="*60)
        print("âœ… Smart Pipeline å®Œæˆ")
        print("="*60 + "\n")
        
        return {
            "job_dir": str(self.job_dir),
            "assets": assets,
            "policies": policies,
            "segments": segments,
            "transcripts": transcripts,
            "vision_meta": vision_meta,
            "shotcards": shotcards
        }
    
    def _build_assets_manifest(self, input_paths: List[str]) -> Dict[str, Any]:
        """æ„å»ºèµ„æºæ¸…å•"""
        videos = []
        audios = []
        
        for path in input_paths:
            p = Path(path)
            
            if not p.exists():
                print(f"âš ï¸  æ–‡ä»¶ä¸å­˜åœ¨: {path}")
                continue
            
            # åˆ¤æ–­æ–‡ä»¶ç±»å‹
            ext = p.suffix.lower()
            
            if ext in ['.mp4', '.mov', '.avi', '.mkv', '.mts', '.m4v']:
                videos.append({
                    "asset_id": f"V{len(videos)+1:03d}",
                    "type": "video",
                    "path": str(p.absolute()),
                    "filename": p.name,
                    "size_mb": p.stat().st_size / (1024*1024)
                })
            
            elif ext in ['.wav', '.mp3', '.aac', '.m4a', '.flac']:
                audios.append({
                    "asset_id": f"A{len(audios)+1:03d}",
                    "type": "audio",
                    "path": str(p.absolute()),
                    "filename": p.name,
                    "size_mb": p.stat().st_size / (1024*1024)
                })
        
        return {
            "videos": videos,
            "audios": audios
        }
    
    def _quick_quality_triage(self, assets: Dict[str, Any]) -> Dict[str, Any]:
        """å¿«é€Ÿè´¨é‡ç­›é€‰ï¼ˆæ— éœ€ AIï¼‰"""
        for video in assets["videos"]:
            # ç®€å•è§„åˆ™ï¼šæ–‡ä»¶å¤§å° < 1MB å¯èƒ½æŸå
            usable = video["size_mb"] >= 1.0
            
            video["quality"] = {
                "usable": usable,
                "reason": "æ–‡ä»¶è¿‡å°" if not usable else "OK"
            }
        
        return assets
    
    def _match_audio_to_video(self, assets: Dict[str, Any]) -> Dict[str, Any]:
        """åŒ¹é…å¤–éƒ¨éŸ³é¢‘åˆ°è§†é¢‘"""
        if not assets["audios"]:
            return assets
        
        matches = self.audio_matcher.match_audio_to_videos(
            assets["videos"],
            assets["audios"]
        )
        
        # æ›´æ–°è§†é¢‘èµ„æº
        for match in matches:
            for video in assets["videos"]:
                if video["asset_id"] == match.video_asset_id:
                    video["matched_audio_asset_id"] = match.audio_asset_id
                    video["audio_match_method"] = match.match_method
                    video["audio_match_confidence"] = match.confidence
                    video["audio_offset_sec"] = match.audio_offset_sec
                    break
        
        return assets
    
    def _decide_modality_policies(self, assets: Dict[str, Any]) -> Dict[str, Any]:
        """å†³å®šæ¯ä¸ªèµ„æºçš„æ¨¡æ€ç­–ç•¥"""
        policies = {}
        
        for video in assets["videos"]:
            if not video.get("quality", {}).get("usable", True):
                policies[video["asset_id"]] = {
                    "mode": "SKIP",
                    "reason": "è´¨é‡ä¸å¯ç”¨"
                }
                continue
            
            # è·å–éŸ³é¢‘è·¯å¾„
            audio_path = None
            if video.get("matched_audio_asset_id"):
                for audio in assets["audios"]:
                    if audio["asset_id"] == video["matched_audio_asset_id"]:
                        audio_path = audio["path"]
                        break
            
            # åˆ†ææ¨¡æ€
            analysis = self.modality_analyzer.analyze(
                video["path"],
                audio_path
            )
            
            policies[video["asset_id"]] = {
                "mode": analysis.recommended_mode,
                "confidence": analysis.confidence,
                "has_voice": analysis.has_voice,
                "speech_ratio": analysis.speech_ratio,
                "likely_talking_head": analysis.likely_talking_head
            }
        
        return policies
    
    def _print_modality_summary(self, policies: Dict[str, Any]):
        """æ‰“å°æ¨¡æ€åˆ†ææ‘˜è¦"""
        mode_counts = {}
        for policy in policies.values():
            mode = policy["mode"]
            mode_counts[mode] = mode_counts.get(mode, 0) + 1
        
        for mode, count in mode_counts.items():
            print(f"  {mode}: {count} ä¸ª")
    
    def _segment_assets(
        self,
        assets: Dict[str, Any],
        policies: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """åˆ†å‰²èµ„æºä¸ºå¯å‰ªè¾‘æ®µ"""
        segments = []
        
        for video in assets["videos"]:
            asset_id = video["asset_id"]
            policy = policies.get(asset_id, {})
            mode = policy.get("mode", "SKIP")
            
            if mode == "SKIP":
                continue
            
            # ç®€åŒ–ç‰ˆï¼šå›ºå®šæ—¶é•¿åˆ†æ®µï¼ˆå®é™…åº”è¯¥ç”¨ VAD æˆ–åœºæ™¯æ£€æµ‹ï¼‰
            # TODO: å®ç°åŸºäº VAD çš„æ™ºèƒ½åˆ†æ®µ
            segments.append({
                "seg_id": f"{asset_id}_S001",
                "asset_id": asset_id,
                "start_sec": 0,
                "end_sec": 999999,  # æ•´ä¸ªè§†é¢‘
                "priority": "high" if mode == "ASR_PRIMARY" else "medium"
            })
        
        return segments
    
    def _run_asr_pass(
        self,
        segments: List[Dict[str, Any]],
        policies: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ASR è¯†åˆ«ï¼ˆä»…å¯¹é€‰å®šæ®µï¼‰"""
        transcripts = {}
        
        for seg in segments:
            asset_id = seg["asset_id"]
            policy = policies.get(asset_id, {})
            mode = policy.get("mode", "SKIP")
            
            # åªå¯¹ ASR_PRIMARY å’Œ HYBRID è¿è¡Œ ASR
            if mode not in ["ASR_PRIMARY", "HYBRID"]:
                continue
            
            # TODO: å®é™…è°ƒç”¨ Whisper ASR
            # è¿™é‡Œç”¨å ä½ç¬¦
            transcripts[seg["seg_id"]] = {
                "text": "[ASR placeholder]",
                "confidence": 0.9,
                "words": []
            }
        
        return transcripts
    
    def _run_vision_pass(
        self,
        segments: List[Dict[str, Any]],
        policies: Dict[str, Any],
        transcripts: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Vision åˆ†æï¼ˆä»…åœ¨å¿…è¦æ—¶ï¼‰"""
        vision_caps = {}
        
        for seg in segments:
            asset_id = seg["asset_id"]
            seg_id = seg["seg_id"]
            policy = policies.get(asset_id, {})
            
            # æ„å»ºæ¨¡æ€åˆ†æå¯¹è±¡ï¼ˆç®€åŒ–ç‰ˆï¼‰
            from .modality_analyzer import ModalityAnalysis
            modality = ModalityAnalysis(
                has_voice=policy.get("has_voice", False),
                speech_ratio=policy.get("speech_ratio", 0.0),
                music_ratio=0.0,
                silence_ratio=1.0 - policy.get("speech_ratio", 0.0),
                likely_talking_head=policy.get("likely_talking_head", False),
                recommended_mode=policy.get("mode", "SKIP"),
                confidence=policy.get("confidence", 0.0),
                audio_present=policy.get("has_voice", False),
                avg_volume_db=-20,
                volume_variance=10,
                speech_segments=0
            )
            
            # åˆ¤æ–­æ˜¯å¦åº”è¯¥è¿è¡Œ Vision
            has_transcript = seg_id in transcripts
            transcript_conf = transcripts.get(seg_id, {}).get("confidence", 1.0)
            
            if should_run_vision(modality, has_transcript, transcript_conf):
                # TODO: å®é™…è°ƒç”¨ Vision åˆ†æ
                # è¿™é‡Œç”¨å ä½ç¬¦
                vision_caps[seg_id] = "[Vision caption placeholder]"
        
        return vision_caps
    
    def _structure_vision_data(self, vision_caps: Dict[str, Any]) -> Dict[str, Any]:
        """ç»“æ„åŒ– Vision æ•°æ®ï¼ˆä½¿ç”¨ LLMï¼‰"""
        vision_meta = {}
        
        for seg_id, caption in vision_caps.items():
            # TODO: å®é™…è°ƒç”¨ DeepSeek ç»“æ„åŒ–
            # è¿™é‡Œç”¨å ä½ç¬¦
            vision_meta[seg_id] = {
                "summary": caption,
                "shot_type": "ä¸­æ™¯",
                "subjects": ["äººç‰©"],
                "mood": "ä¸­æ€§",
                "quality_score": 7
            }
        
        return vision_meta
    
    def _generate_shotcards(
        self,
        segments: List[Dict[str, Any]],
        transcripts: Dict[str, Any],
        vision_meta: Dict[str, Any],
        assets: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """ç”Ÿæˆ ShotCards"""
        shotcards = []
        
        for seg in segments:
            seg_id = seg["seg_id"]
            
            shotcard = {
                "shotcard_id": seg_id,
                "asset_id": seg["asset_id"],
                "start_sec": seg["start_sec"],
                "end_sec": seg["end_sec"],
                "transcript": transcripts.get(seg_id),
                "vision": vision_meta.get(seg_id),
                "usable": True,
                "score": 7.0,
                "intent_tags": [],
                "entities": []
            }
            
            # åº”ç”¨ä¸¢å¼ƒè§„åˆ™
            shotcard = self._apply_drop_rules(shotcard)
            
            if shotcard["usable"]:
                shotcards.append(shotcard)
        
        return shotcards
    
    def _apply_drop_rules(self, shotcard: Dict[str, Any]) -> Dict[str, Any]:
        """åº”ç”¨ä¸¢å¼ƒè§„åˆ™"""
        # è§„åˆ™ 1: æ— å†…å®¹
        if not shotcard["transcript"] and not shotcard["vision"]:
            shotcard["usable"] = False
            shotcard["drop_reason"] = "æ— å†…å®¹"
        
        return shotcard
    
    def _save_json(self, filename: str, data: Any):
        """ä¿å­˜ JSON æ–‡ä»¶"""
        path = self.temp_dir / filename
        with open(path, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)


def run_smart_pipeline(job_dir: Path, input_paths: List[str]) -> Dict[str, Any]:
    """
    å¿«æ·å‡½æ•°ï¼šè¿è¡Œæ™ºèƒ½æµæ°´çº¿
    
    Args:
        job_dir: Job ç›®å½•
        input_paths: è¾“å…¥æ–‡ä»¶è·¯å¾„åˆ—è¡¨
    
    Returns:
        å¤„ç†ç»“æœ
    """
    pipeline = SmartPipeline(job_dir)
    return pipeline.run(input_paths)
