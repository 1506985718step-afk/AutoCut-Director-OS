"""
è§†è§‰åˆ†æå™¨å·¥å‚ - è‡ªåŠ¨é€‰æ‹©æœ€ä½³åˆ†æå™¨

æ ¹æ®é…ç½®è‡ªåŠ¨é€‰æ‹©ï¼š
- æœ¬åœ°æ¨¡å‹ï¼ˆOllamaï¼‰- é›¶æˆæœ¬ï¼Œå¿«é€Ÿï¼Œæ¨è
- äº‘ç«¯æ¨¡å‹ï¼ˆOpenAI GPT-4o Visionï¼‰- é«˜è´¨é‡ï¼Œæœ‰æˆæœ¬

v2.0: é›†æˆ RuntimeProfile å’Œ ExecutionPolicy
"""
from typing import Optional, Literal
from ..config import settings
from ..models.schemas import ScenesJSON


def get_visual_analyzer(
    force_local: Optional[bool] = None,
    force_cloud: Optional[bool] = None,
    model: Optional[str] = None,
    use_policy: bool = True
):
    """
    è·å–è§†è§‰åˆ†æå™¨å®ä¾‹
    
    Args:
        force_local: å¼ºåˆ¶ä½¿ç”¨æœ¬åœ°æ¨¡å‹
        force_cloud: å¼ºåˆ¶ä½¿ç”¨äº‘ç«¯æ¨¡å‹
        model: æŒ‡å®šæ¨¡å‹åç§°ï¼ˆæœ¬åœ°ï¼šmoondream/llava-phi3ï¼Œäº‘ç«¯ï¼šgpt-4oï¼‰
        use_policy: æ˜¯å¦ä½¿ç”¨æ‰§è¡Œç­–ç•¥ï¼ˆæ¨èï¼‰
    
    Returns:
        è§†è§‰åˆ†æå™¨å®ä¾‹
    """
    # ä¼˜å…ˆçº§ï¼šforce_cloud > force_local > ExecutionPolicy > é…ç½®æ–‡ä»¶
    use_local = settings.USE_LOCAL_VISION
    selected_model = model
    local_backend = settings.LOCAL_VISION_PROVIDER  # ollama æˆ– lmstudio
    
    # å¦‚æœå¯ç”¨ç­–ç•¥ï¼Œä» ExecutionPolicy è·å–é…ç½®
    if use_policy and not force_local and not force_cloud and not model:
        try:
            from ..core.execution_policy import get_execution_policy
            policy = get_execution_policy()
            
            use_local = (policy.vision.provider == "local")
            selected_model = policy.vision.model
            local_backend = policy.vision.local_backend or local_backend
            
            print(f"ğŸ“Š ä½¿ç”¨æ‰§è¡Œç­–ç•¥: provider={policy.vision.provider}, backend={local_backend}, model={selected_model}")
        except Exception as e:
            print(f"âš ï¸  æ— æ³•è·å–æ‰§è¡Œç­–ç•¥ï¼Œä½¿ç”¨é»˜è®¤é…ç½®: {e}")
    
    # å¼ºåˆ¶å‚æ•°è¦†ç›–
    if force_cloud:
        use_local = False
    elif force_local:
        use_local = True
    
    if model:
        selected_model = model
    
    if use_local:
        # ä½¿ç”¨æœ¬åœ°æ¨¡å‹
        if local_backend == "lmstudio":
            # ä½¿ç”¨ LM Studio
            from .visual_analyzer_lmstudio import LMStudioVisualAnalyzer
            
            lmstudio_model = selected_model or settings.LMSTUDIO_MODEL
            
            print(f"ğŸ  ä½¿ç”¨ LM Studio è§†è§‰æ¨¡å‹: {lmstudio_model}")
            
            return LMStudioVisualAnalyzer(
                base_url=settings.LMSTUDIO_HOST,
                model=lmstudio_model
            )
        else:
            # ä½¿ç”¨ Ollama
            from .visual_analyzer_local import LocalVisualAnalyzer
            
            local_model = selected_model or settings.LOCAL_VISION_MODEL
            
            # æ£€æŸ¥æ˜¯å¦åº”è¯¥ä½¿ç”¨ CPU æ¨¡å¼
            device = "auto"
            try:
                from ..core.runtime_monitor import get_runtime_monitor
                monitor = get_runtime_monitor()
                if monitor.should_use_cpu_for_vision():
                    device = "cpu"
                    print(f"âš ï¸  èµ„æºç´§å¼ ï¼Œå¼ºåˆ¶ä½¿ç”¨ CPU æ¨¡å¼")
            except:
                pass
            
            print(f"ğŸ  ä½¿ç”¨ Ollama è§†è§‰æ¨¡å‹: {local_model} (device={device})")
            
            return LocalVisualAnalyzer(
                model=local_model,
                ollama_host=settings.OLLAMA_HOST
            )
    else:
        # ä½¿ç”¨äº‘ç«¯æ¨¡å‹
        from .visual_analyzer import VisualAnalyzer
        
        cloud_model = selected_model or "gpt-4o"
        print(f"â˜ï¸  ä½¿ç”¨äº‘ç«¯è§†è§‰æ¨¡å‹: {cloud_model}")
        return VisualAnalyzer()


def analyze_scenes_auto(
    scenes_data: ScenesJSON,
    video_path: str,
    max_scenes: Optional[int] = None,
    force_local: Optional[bool] = None,
    force_cloud: Optional[bool] = None,
    model: Optional[str] = None,
    use_policy: bool = True
) -> ScenesJSON:
    """
    è‡ªåŠ¨é€‰æ‹©æœ€ä½³åˆ†æå™¨è¿›è¡Œåœºæ™¯åˆ†æ
    
    Args:
        scenes_data: åœºæ™¯æ•°æ®
        video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
        max_scenes: é™åˆ¶åˆ†ææ•°é‡ï¼ˆå¦‚æœä¸º Noneï¼Œä» ExecutionPolicy è·å–ï¼‰
        force_local: å¼ºåˆ¶ä½¿ç”¨æœ¬åœ°æ¨¡å‹
        force_cloud: å¼ºåˆ¶ä½¿ç”¨äº‘ç«¯æ¨¡å‹
        model: æŒ‡å®šæ¨¡å‹åç§°
        use_policy: æ˜¯å¦ä½¿ç”¨æ‰§è¡Œç­–ç•¥ï¼ˆæ¨èï¼‰
    
    Returns:
        æ›´æ–°åçš„åœºæ™¯æ•°æ®
    """
    # å¦‚æœæœªæŒ‡å®š max_scenesï¼Œä» ExecutionPolicy è·å–
    if max_scenes is None and use_policy:
        try:
            from ..core.execution_policy import get_execution_policy
            policy = get_execution_policy()
            max_scenes = policy.vision.max_scenes
            print(f"ğŸ“Š ä»æ‰§è¡Œç­–ç•¥è·å– max_scenes: {max_scenes}")
        except:
            pass
    
    analyzer = get_visual_analyzer(
        force_local=force_local,
        force_cloud=force_cloud,
        model=model,
        use_policy=use_policy
    )
    
    # è®°å½•ä»»åŠ¡ç»“æœ
    try:
        result = analyzer.analyze_scene_visuals(
            scenes_data,
            video_path,
            max_scenes
        )
        
        # è®°å½•æˆåŠŸ
        try:
            from ..core.runtime_monitor import get_runtime_monitor
            monitor = get_runtime_monitor()
            monitor.record_task_result(success=True)
        except:
            pass
        
        return result
    
    except Exception as e:
        # è®°å½•å¤±è´¥
        try:
            from ..core.runtime_monitor import get_runtime_monitor
            monitor = get_runtime_monitor()
            monitor.record_task_result(success=False)
        except:
            pass
        
        raise e
