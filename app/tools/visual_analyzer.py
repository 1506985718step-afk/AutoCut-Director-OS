"""
è§†è§‰åˆ†æå™¨ - ç»™ AutoCut Director è£…ä¸Šçœ¼ç›

åŠŸèƒ½ï¼š
1. æˆªå–æ¯ä¸ªåœºæ™¯çš„å…³é”®å¸§
2. è°ƒç”¨ GPT-4o Vision API åˆ†æç”»é¢å†…å®¹ï¼ˆäº‘ç«¯ï¼‰
3. ä¸º scenes.json æ·»åŠ è§†è§‰å…ƒæ•°æ®

æ³¨æ„ï¼šæ¨èä½¿ç”¨ visual_analyzer_local.pyï¼ˆæœ¬åœ°æ¨¡å‹ï¼Œé›¶æˆæœ¬ï¼‰
"""
import base64
import json
import subprocess
import os
import tempfile
from pathlib import Path
from typing import List, Optional

from openai import OpenAI

from ..config import settings
from ..models.schemas import ScenesJSON, VisualMetadata


class VisualAnalyzer:
    """è§†è§‰åˆ†æå™¨ - è®© AI å¯¼æ¼”èƒ½"çœ‹æ‡‚"ç”»é¢"""
    
    def __init__(self):
        """åˆå§‹åŒ–ï¼šå¤ç”¨é…ç½®ä¸­çš„ API Key"""
        if not settings.OPENAI_API_KEY:
            raise ValueError("OPENAI_API_KEY not configured in .env")
        
        client_kwargs = {"api_key": settings.OPENAI_API_KEY}
        if settings.OPENAI_BASE_URL:
            client_kwargs["base_url"] = settings.OPENAI_BASE_URL
        
        self.client = OpenAI(**client_kwargs)
        
        # å¼ºåˆ¶ä½¿ç”¨æ”¯æŒè§†è§‰çš„æ¨¡å‹
        self.vision_model = "gpt-4o"
    
    def _extract_frame_base64(self, video_path: str, time_sec: float) -> Optional[str]:
        """
        ä½¿ç”¨ FFmpeg æˆªå–æŒ‡å®šæ—¶é—´ç‚¹çš„å¸§ï¼Œè¿”å› base64 å­—ç¬¦ä¸²
        
        Args:
            video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
            time_sec: æ—¶é—´ç‚¹ï¼ˆç§’ï¼‰
        
        Returns:
            base64 ç¼–ç çš„å›¾ç‰‡å­—ç¬¦ä¸²ï¼Œå¤±è´¥è¿”å› None
        """
        # ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶
        with tempfile.NamedTemporaryFile(suffix='.jpg', delete=False) as tmp:
            temp_img = tmp.name
        
        try:
            cmd = [
                "ffmpeg",
                "-ss", str(time_sec),
                "-i", video_path,
                "-frames:v", "1",
                "-q:v", "2",  # é«˜è´¨é‡ JPG
                "-y",
                temp_img
            ]
            
            # æ‰§è¡Œæˆªå¸§ï¼ˆé™é»˜æ¨¡å¼ï¼‰
            result = subprocess.run(
                cmd,
                check=True,
                stdout=subprocess.DEVNULL,
                stderr=subprocess.DEVNULL,
                timeout=10
            )
            
            if not os.path.exists(temp_img):
                raise RuntimeError("Frame extraction failed: image not created")
            
            # è¯»å–å¹¶ç¼–ç 
            with open(temp_img, "rb") as image_file:
                return base64.b64encode(image_file.read()).decode('utf-8')
        
        except subprocess.TimeoutExpired:
            print(f"  âš ï¸ æˆªå¸§è¶…æ—¶ ({time_sec}s)")
            return None
        except subprocess.CalledProcessError as e:
            print(f"  âš ï¸ æˆªå¸§å¤±è´¥ ({time_sec}s): FFmpeg error")
            return None
        except Exception as e:
            print(f"  âš ï¸ æˆªå¸§å¤±è´¥ ({time_sec}s): {e}")
            return None
        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if os.path.exists(temp_img):
                try:
                    os.remove(temp_img)
                except:
                    pass
    
    def analyze_scene_visuals(
        self,
        scenes_data: ScenesJSON,
        video_path: str,
        max_scenes: Optional[int] = None
    ) -> ScenesJSON:
        """
        æ‰¹é‡åˆ†æåœºæ™¯çš„è§†è§‰å†…å®¹
        
        Args:
            scenes_data: åœºæ™¯æ•°æ®å¯¹è±¡
            video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
            max_scenes: é™åˆ¶åˆ†ææ•°é‡ï¼ˆè°ƒè¯•ç”¨ï¼‰ï¼ŒNone ä¸ºå…¨éƒ¨åˆ†æ
        
        Returns:
            æ›´æ–°åçš„åœºæ™¯æ•°æ®ï¼ˆåŒ…å« visual å­—æ®µï¼‰
        """
        print(f"\nğŸ‘ï¸  å¼€å§‹è§†è§‰åˆ†æ: {len(scenes_data.scenes)} ä¸ªåœºæ™¯")
        
        if not Path(video_path).exists():
            print(f"  âŒ è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {video_path}")
            return scenes_data
        
        count = 0
        for scene in scenes_data.scenes:
            if max_scenes and count >= max_scenes:
                print(f"\n  â¸ï¸  å·²è¾¾åˆ°é™åˆ¶ ({max_scenes} ä¸ªåœºæ™¯)ï¼Œåœæ­¢åˆ†æ")
                break
            
            # 1. å¦‚æœå·²æœ‰è§†è§‰æ•°æ®ï¼Œè·³è¿‡
            if scene.visual:
                print(f"  â­ï¸  {scene.scene_id} å·²æœ‰è§†è§‰æ•°æ®ï¼Œè·³è¿‡")
                continue
            
            # 2. è®¡ç®—ä¸­é—´æ—¶åˆ»
            mid_frame = (scene.start_frame + scene.end_frame) // 2
            mid_sec = mid_frame / scenes_data.meta.fps
            
            # 3. æˆªå–ä»£è¡¨å¸§
            print(f"  > åˆ†æ {scene.scene_id} (T={mid_sec:.1f}s)...", end="", flush=True)
            img_b64 = self._extract_frame_base64(video_path, mid_sec)
            
            if not img_b64:
                print(" âŒ æˆªå¸§å¤±è´¥")
                continue
            
            # 4. è°ƒç”¨ GPT-4o è¯†å›¾
            try:
                scene.visual = self._call_vision_api(img_b64)
                print(f" âœ… [{scene.visual.shot_type}] {scene.visual.summary}")
                count += 1
            except Exception as e:
                print(f" âŒ API é”™è¯¯: {e}")
        
        print(f"\nâœ… è§†è§‰åˆ†æå®Œæˆ: {count}/{len(scenes_data.scenes)} ä¸ªåœºæ™¯")
        return scenes_data
    
    def _call_vision_api(self, img_b64: str) -> VisualMetadata:
        """
        è°ƒç”¨ Vision æ¨¡å‹åˆ†æå›¾ç‰‡
        
        Args:
            img_b64: base64 ç¼–ç çš„å›¾ç‰‡
        
        Returns:
            è§†è§‰å…ƒæ•°æ®
        
        Raises:
            ValueError: API è¿”å›æ— æ•ˆæ•°æ®
        """
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è§†é¢‘ç´ æåˆ†æå¸ˆã€‚è¯·åˆ†æè¿™å¼ è§†é¢‘æˆªå›¾ï¼Œæå–å…³é”®è§†è§‰ä¿¡æ¯ã€‚

è¯·ä»¥ JSON æ ¼å¼è¿”å›ï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
- summary: ç”»é¢å†…å®¹çš„ä¸€å¥è¯æè¿°ï¼ˆä¸­æ–‡ï¼Œ15å­—ä»¥å†…ï¼‰
- shot_type: æ™¯åˆ«ï¼ˆç‰¹å†™/è¿‘æ™¯/ä¸­æ™¯/å…¨æ™¯/è¿œæ™¯ï¼‰
- subjects: ç”»é¢ä¸­çš„ä¸»è¦ç‰©ä½“æˆ–äººç‰©ï¼ˆåˆ—è¡¨ï¼Œå¦‚ ["äººç‰©", "æ‰‹æœº"]ï¼‰
- action: ä¸»ä½“çš„åŠ¨ä½œæˆ–çŠ¶æ€ï¼ˆå¦‚ "è¯´è¯"ã€"è·‘æ­¥"ã€"é™æ­¢"ï¼‰
- mood: ç”»é¢ä¼ è¾¾çš„æƒ…ç»ªï¼ˆå¦‚ "å¼€å¿ƒ"ã€"ç´§å¼ "ã€"å¹³é™"ã€"ç§‘æŠ€æ„Ÿ"ï¼‰
- lighting: å…‰çº¿æƒ…å†µï¼ˆå¦‚ "è‡ªç„¶å…‰"ã€"å®¤å†…"ã€"æš—è°ƒ"ã€"è¿‡æ›"ï¼‰
- quality_score: ç”»é¢è´¨é‡è¯„åˆ† 1-10ï¼ˆè€ƒè™‘æ¸…æ™°åº¦ã€æ„å›¾ã€ç¾æ„Ÿï¼‰

ç¤ºä¾‹è¾“å‡ºï¼š
{
  "summary": "å¹´è½»äººåœ¨å’–å•¡å…ä½¿ç”¨ç¬”è®°æœ¬ç”µè„‘",
  "shot_type": "ä¸­æ™¯",
  "subjects": ["äººç‰©", "ç¬”è®°æœ¬ç”µè„‘", "å’–å•¡æ¯"],
  "action": "å·¥ä½œ",
  "mood": "ä¸“æ³¨",
  "lighting": "è‡ªç„¶å…‰",
  "quality_score": 8
}"""
        
        try:
            response = self.client.chat.completions.create(
                model=self.vision_model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/jpeg;base64,{img_b64}",
                                    "detail": "low"  # ä½¿ç”¨ low é™ä½æˆæœ¬
                                }
                            }
                        ]
                    }
                ],
                response_format={"type": "json_object"},
                temperature=0.3,
                max_tokens=300
            )
            
            content = response.choices[0].message.content
            data = json.loads(content)
            
            return VisualMetadata(**data)
        
        except json.JSONDecodeError as e:
            raise ValueError(f"Failed to parse vision response: {e}")
        except Exception as e:
            raise ValueError(f"Vision API error: {e}")


# ä¾¿æ·å‡½æ•°
def analyze_scenes_with_vision(
    scenes_data: ScenesJSON,
    video_path: str,
    max_scenes: Optional[int] = None
) -> ScenesJSON:
    """
    ä¾¿æ·å‡½æ•°ï¼šä¸ºåœºæ™¯æ•°æ®æ·»åŠ è§†è§‰åˆ†æ
    
    Args:
        scenes_data: åœºæ™¯æ•°æ®
        video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
        max_scenes: é™åˆ¶åˆ†ææ•°é‡ï¼ˆå¯é€‰ï¼‰
    
    Returns:
        æ›´æ–°åçš„åœºæ™¯æ•°æ®
    """
    analyzer = VisualAnalyzer()
    return analyzer.analyze_scene_visuals(scenes_data, video_path, max_scenes)


# ç‹¬ç«‹æµ‹è¯•å…¥å£
if __name__ == "__main__":
    import sys
    from pathlib import Path
    
    # ç”¨æ³•: python -m app.tools.visual_analyzer video.mp4 scenes.json
    if len(sys.argv) < 3:
        print("ç”¨æ³•: python -m app.tools.visual_analyzer <video.mp4> <scenes.json>")
        sys.exit(1)
    
    video_path = sys.argv[1]
    scenes_path = sys.argv[2]
    
    if not Path(video_path).exists():
        print(f"âŒ è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {video_path}")
        sys.exit(1)
    
    if not Path(scenes_path).exists():
        print(f"âŒ åœºæ™¯æ–‡ä»¶ä¸å­˜åœ¨: {scenes_path}")
        sys.exit(1)
    
    # åŠ è½½ scenes.json
    with open(scenes_path, 'r', encoding='utf-8') as f:
        scenes_dict = json.load(f)
    
    scenes_data = ScenesJSON(**scenes_dict)
    
    # åˆ†æè§†è§‰
    analyzer = VisualAnalyzer()
    updated_scenes = analyzer.analyze_scene_visuals(scenes_data, video_path, max_scenes=5)
    
    # ä¿å­˜ç»“æœ
    output_path = scenes_path.replace('.json', '_with_visual.json')
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(updated_scenes.model_dump(), f, indent=2, ensure_ascii=False)
    
    print(f"\nâœ… ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
