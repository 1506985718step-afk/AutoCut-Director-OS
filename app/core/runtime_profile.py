"""
Runtime Profile - è¿è¡Œæ—¶é…ç½®æ–‡ä»¶

åŠŸèƒ½ï¼š
1. è‡ªåŠ¨æ£€æµ‹ç¡¬ä»¶é…ç½®
2. ç”Ÿæˆè¿è¡Œæ—¶ Profile
3. åŠ¨æ€ç›‘æ§ç³»ç»ŸçŠ¶æ€
4. è‡ªé€‚åº”é™çº§ç­–ç•¥

æ ¸å¿ƒåè®®ï¼šè®©ç³»ç»Ÿ"çŸ¥é“è‡ªå·±åœ¨å¹²ä»€ä¹ˆ"
"""
import psutil
import platform
from typing import Dict, Any, Optional, Literal
from dataclasses import dataclass, asdict
from pathlib import Path
import json


@dataclass
class CPUProfile:
    """CPU é…ç½®"""
    cores: int
    threads: int
    score: Literal["low", "medium", "high", "ultra"]
    
    @classmethod
    def detect(cls) -> "CPUProfile":
        """è‡ªåŠ¨æ£€æµ‹ CPU"""
        cores = psutil.cpu_count(logical=False) or 4
        threads = psutil.cpu_count(logical=True) or 8
        
        # è¯„åˆ†è§„åˆ™
        if threads >= 16:
            score = "ultra"
        elif threads >= 12:
            score = "high"
        elif threads >= 8:
            score = "medium"
        else:
            score = "low"
        
        return cls(cores=cores, threads=threads, score=score)


@dataclass
class MemoryProfile:
    """å†…å­˜é…ç½®"""
    total_gb: float
    available_gb: float
    
    @classmethod
    def detect(cls) -> "MemoryProfile":
        """è‡ªåŠ¨æ£€æµ‹å†…å­˜"""
        memory = psutil.virtual_memory()
        total_gb = memory.total / (1024**3)
        available_gb = memory.available / (1024**3)
        
        return cls(total_gb=round(total_gb, 1), available_gb=round(available_gb, 1))


@dataclass
class GPUProfile:
    """GPU é…ç½®"""
    vendor: str
    model: str
    vram_gb: float
    cuda: bool
    
    @classmethod
    def detect(cls) -> Optional["GPUProfile"]:
        """è‡ªåŠ¨æ£€æµ‹ GPU"""
        try:
            import GPUtil
            gpus = GPUtil.getGPUs()
            
            if gpus:
                gpu = gpus[0]
                return cls(
                    vendor="NVIDIA",
                    model=gpu.name,
                    vram_gb=round(gpu.memoryTotal / 1024, 1),
                    cuda=True
                )
        except:
            pass
        
        # å°è¯•æ£€æµ‹ AMD/Intel
        try:
            import wmi
            w = wmi.WMI()
            for gpu in w.Win32_VideoController():
                if "AMD" in gpu.Name or "Radeon" in gpu.Name:
                    return cls(
                        vendor="AMD",
                        model=gpu.Name,
                        vram_gb=0.0,  # æ— æ³•å‡†ç¡®è·å–
                        cuda=False
                    )
                elif "Intel" in gpu.Name:
                    return cls(
                        vendor="Intel",
                        model=gpu.Name,
                        vram_gb=0.0,
                        cuda=False
                    )
        except:
            pass
        
        return None


@dataclass
class AIRuntimeProfile:
    """AI è¿è¡Œæ—¶é…ç½®"""
    ollama: bool
    ollama_models: list[str]
    lmstudio: bool
    lmstudio_model: Optional[str]
    cuda_available: bool
    
    @classmethod
    def detect(cls) -> "AIRuntimeProfile":
        """è‡ªåŠ¨æ£€æµ‹ AI è¿è¡Œæ—¶"""
        import requests
        
        # æ£€æµ‹ Ollama
        ollama = False
        ollama_models = []
        
        try:
            response = requests.get("http://localhost:11434/api/tags", timeout=2)
            if response.status_code == 200:
                ollama = True
                models = response.json().get("models", [])
                ollama_models = [m.get("name", "").split(":")[0] for m in models]
        except:
            pass
        
        # æ£€æµ‹ LM Studio
        lmstudio = False
        lmstudio_model = None
        
        try:
            response = requests.get("http://localhost:1234/v1/models", timeout=2)
            if response.status_code == 200:
                lmstudio = True
                data = response.json()
                models = data.get("data", [])
                if models:
                    lmstudio_model = models[0].get("id", "unknown")
        except:
            pass
        
        # æ£€æµ‹ CUDA
        cuda_available = False
        try:
            import torch
            cuda_available = torch.cuda.is_available()
        except:
            pass
        
        return cls(
            ollama=ollama,
            ollama_models=ollama_models,
            lmstudio=lmstudio,
            lmstudio_model=lmstudio_model,
            cuda_available=cuda_available
        )


@dataclass
class EditorProfile:
    """ç¼–è¾‘å™¨é…ç½®"""
    davinci: Dict[str, Any]
    
    @classmethod
    def detect(cls) -> "EditorProfile":
        """è‡ªåŠ¨æ£€æµ‹ç¼–è¾‘å™¨"""
        davinci = {
            "installed": False,
            "version": None,
            "scriptable": False
        }
        
        # æ£€æµ‹ DaVinci Resolve
        try:
            import sys
            sys.path.append(r"C:\ProgramData\Blackmagic Design\DaVinci Resolve\Support\Developer\Scripting\Modules")
            import DaVinciResolveScript as dvr
            
            resolve = dvr.scriptapp("Resolve")
            if resolve:
                davinci["installed"] = True
                davinci["scriptable"] = True
                # å°è¯•è·å–ç‰ˆæœ¬
                try:
                    version = resolve.GetVersion()
                    davinci["version"] = version
                except:
                    davinci["version"] = "unknown"
        except:
            pass
        
        return cls(davinci=davinci)


@dataclass
class RuntimeProfile:
    """å®Œæ•´çš„è¿è¡Œæ—¶é…ç½®æ–‡ä»¶"""
    cpu: CPUProfile
    memory: MemoryProfile
    gpu: Optional[GPUProfile]
    ai_runtime: AIRuntimeProfile
    editor: EditorProfile
    os: str
    profile_class: Literal[
        "LOCAL_GPU_HIGH",    # 4090 å·¥ä½œç«™
        "LOCAL_GPU_MID",     # 4060/3060 çº§åˆ«
        "LOCAL_GPU_LOW",     # 1660/2060 çº§åˆ«
        "LOCAL_CPU_ONLY",    # æ— ç‹¬æ˜¾
        "CLOUD_HYBRID"       # æ··åˆæ¨¡å¼
    ]
    degraded: bool = False  # æ˜¯å¦å·²é™çº§
    degradation_reason: Optional[str] = None
    
    @classmethod
    def detect(cls) -> "RuntimeProfile":
        """è‡ªåŠ¨æ£€æµ‹å®Œæ•´é…ç½®"""
        cpu = CPUProfile.detect()
        memory = MemoryProfile.detect()
        gpu = GPUProfile.detect()
        ai_runtime = AIRuntimeProfile.detect()
        editor = EditorProfile.detect()
        os_name = platform.system()
        
        # åˆ¤æ–­ profile_class
        profile_class = cls._classify_profile(cpu, memory, gpu, ai_runtime)
        
        return cls(
            cpu=cpu,
            memory=memory,
            gpu=gpu,
            ai_runtime=ai_runtime,
            editor=editor,
            os=os_name,
            profile_class=profile_class
        )
    
    @staticmethod
    def _classify_profile(
        cpu: CPUProfile,
        memory: MemoryProfile,
        gpu: Optional[GPUProfile],
        ai_runtime: AIRuntimeProfile
    ) -> str:
        """åˆ†ç±» Profile"""
        if not gpu or not gpu.cuda:
            return "LOCAL_CPU_ONLY"
        
        # æ ¹æ®æ˜¾å­˜åˆ†ç±»
        if gpu.vram_gb >= 16:
            return "LOCAL_GPU_HIGH"
        elif gpu.vram_gb >= 8:
            return "LOCAL_GPU_MID"
        elif gpu.vram_gb >= 4:
            return "LOCAL_GPU_LOW"
        else:
            return "LOCAL_CPU_ONLY"
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return {
            "cpu": asdict(self.cpu),
            "memory": asdict(self.memory),
            "gpu": asdict(self.gpu) if self.gpu else None,
            "ai_runtime": asdict(self.ai_runtime),
            "editor": asdict(self.editor),
            "os": self.os,
            "profile_class": self.profile_class,
            "degraded": self.degraded,
            "degradation_reason": self.degradation_reason
        }
    
    def save(self, path: Path):
        """ä¿å­˜åˆ°æ–‡ä»¶"""
        with open(path, 'w', encoding='utf-8') as f:
            json.dump(self.to_dict(), f, indent=2, ensure_ascii=False)
    
    @classmethod
    def load(cls, path: Path) -> "RuntimeProfile":
        """ä»æ–‡ä»¶åŠ è½½"""
        with open(path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        return cls(
            cpu=CPUProfile(**data["cpu"]),
            memory=MemoryProfile(**data["memory"]),
            gpu=GPUProfile(**data["gpu"]) if data["gpu"] else None,
            ai_runtime=AIRuntimeProfile(**data["ai_runtime"]),
            editor=EditorProfile(**data["editor"]),
            os=data["os"],
            profile_class=data["profile_class"],
            degraded=data.get("degraded", False),
            degradation_reason=data.get("degradation_reason")
        )
    
    def mark_degraded(self, reason: str):
        """æ ‡è®°ä¸ºå·²é™çº§"""
        self.degraded = True
        self.degradation_reason = reason
    
    def get_explanation(self) -> str:
        """ç”Ÿæˆç”¨æˆ·å‹å¥½çš„è§£é‡Š"""
        lines = ["ğŸ§  ç³»ç»Ÿè¿è¡Œæ¨¡å¼"]
        
        # ç¡¬ä»¶è¯´æ˜
        if self.gpu and self.gpu.cuda:
            lines.append(f"- æ£€æµ‹åˆ° {self.gpu.vendor} {self.gpu.model} ({self.gpu.vram_gb}GB æ˜¾å­˜)")
        else:
            lines.append("- æœªæ£€æµ‹åˆ°ç‹¬ç«‹æ˜¾å¡")
        
        lines.append(f"- CPU: {self.cpu.threads} çº¿ç¨‹ ({self.cpu.score} æ€§èƒ½)")
        lines.append(f"- å†…å­˜: {self.memory.total_gb}GB (å¯ç”¨ {self.memory.available_gb}GB)")
        
        # AI è¿è¡Œæ—¶
        if self.ai_runtime.ollama:
            lines.append(f"- æœ¬åœ° AI (Ollama): {len(self.ai_runtime.ollama_models)} ä¸ªæ¨¡å‹")
        elif self.ai_runtime.lmstudio:
            lines.append(f"- æœ¬åœ° AI (LM Studio): {self.ai_runtime.lmstudio_model}")
        else:
            lines.append("- æœ¬åœ° AI: æœªå®‰è£…")
        
        # é™çº§è¯´æ˜
        if self.degraded:
            lines.append(f"\nâš ï¸  å·²è‡ªåŠ¨é™çº§: {self.degradation_reason}")
        
        # è¿è¡Œç­–ç•¥
        lines.append(f"\nğŸ“Š è¿è¡Œçº§åˆ«: {self.profile_class}")
        
        return "\n".join(lines)


# å…¨å±€å•ä¾‹
_runtime_profile: Optional[RuntimeProfile] = None


def get_runtime_profile(force_reload: bool = False) -> RuntimeProfile:
    """è·å–è¿è¡Œæ—¶é…ç½®æ–‡ä»¶ï¼ˆå•ä¾‹ï¼‰"""
    global _runtime_profile
    
    if _runtime_profile is None or force_reload:
        _runtime_profile = RuntimeProfile.detect()
    
    return _runtime_profile


def save_runtime_profile(path: Path):
    """ä¿å­˜è¿è¡Œæ—¶é…ç½®æ–‡ä»¶"""
    profile = get_runtime_profile()
    profile.save(path)


def load_runtime_profile(path: Path) -> RuntimeProfile:
    """åŠ è½½è¿è¡Œæ—¶é…ç½®æ–‡ä»¶"""
    global _runtime_profile
    _runtime_profile = RuntimeProfile.load(path)
    return _runtime_profile
